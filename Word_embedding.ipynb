{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word_embedding.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/abunickabhi/IRproject/blob/master/Word_embedding.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "nqGwS5O9efPe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "outputId": "cd7d5df4-2f91-4e66-9ac2-d548dc41a419"
      },
      "cell_type": "code",
      "source": [
        "!pip3 install Cython\n",
        "!pip3 install polyglot\n",
        "!pip3 install word2vec\n",
        "!pip3 install fasttext\n",
        "!pip3 install gensim\n",
        "!pip3 install torch"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: polyglot in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: word2vec in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from word2vec)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from word2vec)\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy>=1 in /usr/local/lib/python3.6/dist-packages (from fasttext)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from fasttext)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: botocore<1.11.0,>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.11.0,>=1.10.4->boto3->smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: python-dateutil<2.7.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.11.0,>=1.10.4->boto3->smart-open>=1.2.1->gensim)\n",
            "Collecting torch\n",
            "  Downloading torch-0.3.1-cp36-cp36m-manylinux1_x86_64.whl (496.4MB)\n",
            "\u001b[K    13% |████▌                           | 69.4MB 48.7MB/s eta 0:00:09"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[K    100% |████████████████████████████████| 496.4MB 2.5kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch)\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-0.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "adv7d_lwmRfi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Importing all the required libraries**"
      ]
    },
    {
      "metadata": {
        "id": "gr0ljkD8YuQq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import polyglot\n",
        "import word2vec\n",
        "import fasttext\n",
        "import numpy as np\n",
        "from gensim.models import fasttext\n",
        "#from polyglot.text import Text, Word"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mATomXqcldnD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Word2Vec Training**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "_rxeMsCOhu9r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "9a6abf8e-9a10-42e3-ecca-b6fa3040c05d"
      },
      "cell_type": "code",
      "source": [
        "model = word2vec(size=100, window=5, min_count=5, workers=4)\n",
        "model.save('/home/student/Documents/Dataset_Abhinav/english.vec')\n",
        "model = word2vec.load('/home/student/Documents/Dataset_Abhinav/english.vec')\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-830d3f6904bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/student/Documents/Dataset_Abhinav/english.vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/student/Documents/Dataset_Abhinav/english.vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "aQP5PodvmcFE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dUSdLmj3o5qz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "f9c89319-aaf5-4908-c411-ffc9d70b064f"
      },
      "cell_type": "code",
      "source": [
        "max_vocab = 100000\n",
        "emb_dim = 300\n",
        "src_lang = 'en'\n",
        "src_emd_path = '/home/student/Documents/Dataset_Abhinav/english.vec'\n",
        "\n",
        "src_word2id = {}\n",
        "src_embeddings = []\n",
        "\n",
        "with open(src_emd_path) as f:\n",
        "    for i,line in enumerate(f):\n",
        "        if i==0:\n",
        "            split = line.split()\n",
        "            assert len(split) == 2\n",
        "            assert emb_dim == int(split[1])\n",
        "        else:\n",
        "            word, vect = line.rstrip().split(' ', 1)\n",
        "            vect = np.fromstring(vect, sep=' ')\n",
        "            if np.linalg.norm(vect)==0: #to avoid null embeddings\n",
        "                vect[0] = 0.01\n",
        "            assert word not in src_word2id\n",
        "            assert vect.shape == (emb_dim, )\n",
        "            src_word2id[word] = len(src_word2id)\n",
        "            src_embeddings.append(vect[None,:])\n",
        "        if i > max_vocab:\n",
        "            break\n",
        "            \n",
        "src_id2word = {}\n",
        "src_id2word = {v: k for k,v in src_word2id.items()}\n",
        "src_embeddings = np.concatenate(src_embeddings,0)\n",
        "src_embeddings = torch.from_numpy(src_embeddings).float()\n",
        "src_embeddings = src_embeddings.cuda()\n",
        "src_emb = nn.Embedding(len(src_word2id), emb_dim, sparse=True)\n",
        "src_emb.weight.data.copy_(src_embeddings)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-54f42e3d81e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msrc_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_emd_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/wiki.en.vec'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "5e41SBCbpJos",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "outputId": "c6125ff8-4abe-4bc2-ede1-bd2e4c2feecc"
      },
      "cell_type": "code",
      "source": [
        "from src.dictionary import Dictionary\n",
        "src_dico = Dictionary(src_id2word, src_word2id, src_lang)\n",
        "tgt_lang = 'ta'\n",
        "tgt_emd_path = '/home/student/Documents/Dataset_Abhinav/tamil.vec'\n",
        "\n",
        "\n",
        "tgt_word2id = {}\n",
        "tgt_id2word = {}\n",
        "tgt_embeddings = []\n",
        "\n",
        "\n",
        "\n",
        "with open(tgt_emd_path) as f:\n",
        "    for i,line in enumerate(f):\n",
        "        if i==0:\n",
        "            split = line.split()\n",
        "            assert len(split) == 2\n",
        "            assert emb_dim == int(split[1])\n",
        "        else:\n",
        "            word, vect = line.rstrip().split(' ', 1)\n",
        "            vect = np.fromstring(vect, sep=' ')\n",
        "            if np.linalg.norm(vect)==0: #to avoid null embeddings\n",
        "                vect[0] = 0.01\n",
        "            assert word not in tgt_word2id\n",
        "            assert vect.shape == (emb_dim, )\n",
        "            tgt_word2id[word] = len(tgt_word2id)\n",
        "            tgt_embeddings.append(vect[None,:])\n",
        "        if i > max_vocab:\n",
        "            break\n",
        "\n",
        "tgt_id2word = {v:k for k,v in tgt_word2id.items()}\n",
        "\n",
        "tgt_dico = Dictionary(tgt_id2word, tgt_word2id, tgt_lang)\n",
        "tgt_embeddings = np.concatenate(tgt_embeddings,0)\n",
        "tgt_embeddings = torch.from_numpy(tgt_embeddings).float()\n",
        "tgt_embeddings = tgt_embeddings.cuda()\n",
        "\n",
        "tgt_emb = nn.Embedding(len(tgt_word2id), emb_dim, sparse=True)\n",
        "tgt_emb.weight.data.copy_(tgt_embeddings)\n",
        "\n",
        "mapping = nn.Linear(emb_dim, emb_dim, bias=False)\n",
        "mapping.weight.data.copy_(torch.diag(torch.ones(emb_dim)))\n",
        "\n",
        "disc_layers = 2\n",
        "disc_dim_hidden = 2048\n",
        "disc_dropout = 0\n",
        "disc_inp_dropout = 0.1"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-fde37a7408e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msrc_dico\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_id2word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_word2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtgt_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ta'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtgt_emd_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/student/Documents/Dataset_Abhinav/tamil.vec'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "s7DTT7W5pmik",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "26a88c7f-504d-44c2-ed08-77f44e43a5e3"
      },
      "cell_type": "code",
      "source": [
        "'''class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        \n",
        "        self.emb_dim = emb_dim\n",
        "        self.disc_layers = disc_layers\n",
        "        self.disc_dim_hidden = disc_dim_hidden\n",
        "        self.disc_dropout = disc_dropout\n",
        "        self.disc_inp_dropout = disc_inp_dropout\n",
        "        \n",
        "        layers = [nn.Dropout(self.disc_inp_dropout)]\n",
        "        for i in range(self.disc_layers + 1):\n",
        "            input_dim = self.emb_dim if i == 0 else self.disc_dim_hidden\n",
        "            output_dim = 1 if i==self.disc_layers else self.disc_dim_hidden\n",
        "            layers.append(nn.Linear(input_dim, output_dim))\n",
        "            if i < self.disc_layers:\n",
        "                layers.append(nn.LeakyReLU(0.2))\n",
        "                layers.append(nn.Dropout(self.disc_dropout))\n",
        "        layers.append(nn.Sigmoid())\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.layers(x).view(-1)\n",
        "      \n",
        "discriminator = Discriminator()\n",
        "src_emb.cuda()\n",
        "tgt_emb.cuda()\n",
        "mapping.cuda()\n",
        "discriminator.cuda()'''"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'class Discriminator(nn.Module):\\n    def __init__(self):\\n        super(Discriminator, self).__init__()\\n        \\n        self.emb_dim = emb_dim\\n        self.disc_layers = disc_layers\\n        self.disc_dim_hidden = disc_dim_hidden\\n        self.disc_dropout = disc_dropout\\n        self.disc_inp_dropout = disc_inp_dropout\\n        \\n        layers = [nn.Dropout(self.disc_inp_dropout)]\\n        for i in range(self.disc_layers + 1):\\n            input_dim = self.emb_dim if i == 0 else self.disc_dim_hidden\\n            output_dim = 1 if i==self.disc_layers else self.disc_dim_hidden\\n            layers.append(nn.Linear(input_dim, output_dim))\\n            if i < self.disc_layers:\\n                layers.append(nn.LeakyReLU(0.2))\\n                layers.append(nn.Dropout(self.disc_dropout))\\n        layers.append(nn.Sigmoid())\\n        self.layers = nn.Sequential(*layers)\\n        \\n    def forward(self, x):\\n        return self.layers(x).view(-1)\\n      \\ndiscriminator = Discriminator()\\nsrc_emb.cuda()\\ntgt_emb.cuda()\\nmapping.cuda()\\ndiscriminator.cuda()'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "metadata": {
        "id": "WlR41Zw0t3xe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}