{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word_embedding.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/abunickabhi/IRproject/blob/master/Word_embedding.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "nqGwS5O9efPe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        },
        "outputId": "47c6eb1b-cc7e-4061-ba39-6692f43dcbb8"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/abunickabhi/IRproject.git ; rm -rf embed/ ;mkdir embed ;cd IRproject/ ; git pull origin\n",
        "!pip3 install Cython\n",
        "!pip3 install word2vec\n",
        "!pip3 install fasttext\n",
        "!pip3 install gensim\n",
        "!pip3 install torch\n",
        "!pip3 install PyDictionary\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'IRproject' already exists and is not an empty directory.\n",
            "remote: Counting objects: 4, done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 4 (delta 1), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (4/4), done.\n",
            "From https://github.com/abunickabhi/IRproject\n",
            "   58bea75..0744332  master     -> origin/master\n",
            "Updating 58bea75..0744332\n",
            "Fast-forward\n",
            " med_eng.vec | 333 \u001b[32m++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[m\n",
            " med_tml.vec | 400 \u001b[32m++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\u001b[m\n",
            " 2 files changed, 733 insertions(+)\n",
            " create mode 100644 med_eng.vec\n",
            " create mode 100644 med_tml.vec\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: word2vec in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from word2vec)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from word2vec)\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from fasttext)\n",
            "Requirement already satisfied: numpy>=1 in /usr/local/lib/python3.6/dist-packages (from fasttext)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim)\n",
            "Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: botocore<1.11.0,>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.11.0,>=1.10.4->boto3->smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: python-dateutil<2.7.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.11.0,>=1.10.4->boto3->smart-open>=1.2.1->gensim)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from torch)\n",
            "Requirement already satisfied: PyDictionary in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from PyDictionary)\n",
            "Requirement already satisfied: goslate in /usr/local/lib/python3.6/dist-packages (from PyDictionary)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from PyDictionary)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from PyDictionary)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->PyDictionary)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->PyDictionary)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->PyDictionary)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->PyDictionary)\n",
            "Requirement already satisfied: futures in /usr/local/lib/python3.6/dist-packages (from goslate->PyDictionary)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "adv7d_lwmRfi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Importing all the required libraries**"
      ]
    },
    {
      "metadata": {
        "id": "gr0ljkD8YuQq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import word2vec\n",
        "import fasttext\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "from PyDictionary import PyDictionary\n",
        "from torch import optim\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mATomXqcldnD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Loading and Processing the source words i.e. English**\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "dUSdLmj3o5qz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "c5ce8b72-a010-4578-b025-0755277a766e"
      },
      "cell_type": "code",
      "source": [
        "max_vocab = 10000\n",
        "emb_dim = 300\n",
        "src_lang = 'eng'\n",
        "\n",
        "src_word2id = {}\n",
        "src_embeddings = []\n",
        "a= 'IRproject/med_eng.vec'\n",
        "\n",
        "with open(a) as f:\n",
        "    for i,line in enumerate(f):\n",
        "        if i==0:\n",
        "            split = line.split()\n",
        "            assert len(split) == 2\n",
        "            assert emb_dim == int(split[1])\n",
        "        else:\n",
        "            word, vect = line.rstrip().split(' ', 1)\n",
        "            vect = np.fromstring(vect, sep=' ')\n",
        "            if np.linalg.norm(vect)==0: #to avoid null embeddings\n",
        "                vect[0] = 0.01\n",
        "            assert word not in src_word2id\n",
        "            assert vect.shape == (emb_dim, )\n",
        "            src_word2id[word] = len(src_word2id)\n",
        "            src_embeddings.append(vect[None,:])\n",
        "        if i > max_vocab:\n",
        "            break\n",
        "            \n",
        "src_id2word = {v: k for k,v in src_word2id.items()}\n",
        "src_embeddings = np.concatenate(src_embeddings,0)\n",
        "src_embeddings = torch.from_numpy(src_embeddings).float()\n",
        "src_emb = nn.Embedding(len(src_word2id), emb_dim, sparse=True)\n",
        "src_emb.weight.data.copy_(src_embeddings)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              " 0.1073  0.0089  0.0006  ...   0.0050  0.1173 -0.0400\n",
              " 0.0897  0.0160 -0.0571  ...   0.1559 -0.0254 -0.0259\n",
              " 0.0004  0.0032 -0.0204  ...   0.2070  0.0689 -0.0467\n",
              "          ...             ⋱             ...          \n",
              "-0.0988 -0.0694  0.0330  ...   0.1917  0.0759 -0.0402\n",
              " 0.1017 -0.0606 -0.0056  ...   0.1346  0.1418 -0.0015\n",
              " 0.0682  0.0611 -0.0201  ...  -0.1313  0.0914  0.0037\n",
              "[torch.FloatTensor of size 332x300]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "metadata": {
        "id": "XQK9ppSIklmb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Loading the source language i.e. Tamil into a Dictionary**"
      ]
    },
    {
      "metadata": {
        "id": "5e41SBCbpJos",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "ed89856b-6083-45ca-ab4a-c5a86dde4472"
      },
      "cell_type": "code",
      "source": [
        "src_dico = PyDictionary(src_id2word, src_word2id, src_lang)\n",
        "\n",
        "tgt_lang = 'ta'\n",
        "tgt_word2id = {}\n",
        "tgt_embeddings = []\n",
        "b= 'IRproject/med_tml.vec'\n",
        "\n",
        "with open(b) as f:\n",
        "    for i,line in enumerate(f):\n",
        "        if i==0:\n",
        "            split = line.split()\n",
        "            assert len(split) == 2\n",
        "            assert emb_dim == int(split[1])\n",
        "        else:\n",
        "            word, vect = line.rstrip().split(' ', 1)\n",
        "            vect = np.fromstring(vect, sep=' ')\n",
        "            if np.linalg.norm(vect)==0: #to avoid null embeddings\n",
        "                vect[0] = 0.01\n",
        "            assert word not in tgt_word2id\n",
        "            assert vect.shape == (emb_dim, )\n",
        "            tgt_word2id[word] = len(tgt_word2id)\n",
        "            tgt_embeddings.append(vect[None,:])\n",
        "        if i > max_vocab:\n",
        "            break\n",
        "\n",
        "tgt_id2word = {v:k for k,v in tgt_word2id.items()}\n",
        "\n",
        "tgt_dico = PyDictionary(tgt_id2word, tgt_word2id, tgt_lang)\n",
        "tgt_embeddings = np.concatenate(tgt_embeddings,0)\n",
        "tgt_embeddings = torch.from_numpy(tgt_embeddings).float()\n",
        "tgt_emb = nn.Embedding(len(tgt_word2id), emb_dim, sparse=True)\n",
        "tgt_emb.weight.data.copy_(tgt_embeddings)\n",
        "\n",
        "\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "-0.0051 -0.0092  0.0438  ...  -0.0510  0.0875  0.0016\n",
              " 0.0472 -0.0408  0.0252  ...   0.1013  0.0901  0.0136\n",
              "-0.0163  0.0031  0.0754  ...  -0.0167  0.1091 -0.0434\n",
              "          ...             ⋱             ...          \n",
              " 0.0262 -0.0360 -0.0021  ...  -0.0091  0.0209 -0.0222\n",
              "-0.0092 -0.0611 -0.0357  ...  -0.0284 -0.0225 -0.0451\n",
              "-0.0156 -0.0003  0.0217  ...  -0.0131  0.0056  0.0035\n",
              "[torch.FloatTensor of size 399x300]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "metadata": {
        "id": "xT8Ycjckq0Q-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "48e7a266-317a-42cb-c46d-1068756eb370"
      },
      "cell_type": "code",
      "source": [
        "mapping = nn.Linear(emb_dim, emb_dim, bias=False)\n",
        "mapping.weight.data.copy_(torch.diag(torch.ones(emb_dim)))\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    1     0     0  ...      0     0     0\n",
              "    0     1     0  ...      0     0     0\n",
              "    0     0     1  ...      0     0     0\n",
              "       ...          ⋱          ...       \n",
              "    0     0     0  ...      1     0     0\n",
              "    0     0     0  ...      0     1     0\n",
              "    0     0     0  ...      0     0     1\n",
              "[torch.FloatTensor of size 300x300]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "metadata": {
        "id": "d8PAfLi48Lxs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Create a self class in Discriminator class and define the parameters of training**"
      ]
    },
    {
      "metadata": {
        "id": "s7DTT7W5pmik",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "f51dc5b2-30bf-4f29-84a0-268d30d9f829"
      },
      "cell_type": "code",
      "source": [
        "disc_layers = 2\n",
        "disc_dim_hidden = 2048\n",
        "disc_dropout = 0\n",
        "disc_inp_dropout = 0.1\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        \n",
        "        self.emb_dim = emb_dim\n",
        "        self.disc_layers = disc_layers\n",
        "        self.disc_dim_hidden = disc_dim_hidden\n",
        "        self.disc_dropout = disc_dropout\n",
        "        self.disc_inp_dropout = disc_inp_dropout\n",
        "        \n",
        "        layers = [nn.Dropout(self.disc_inp_dropout)]\n",
        "        for i in range(self.disc_layers + 1):\n",
        "            input_dim = self.emb_dim if i == 0 else self.disc_dim_hidden\n",
        "            output_dim = 1 if i==self.disc_layers else self.disc_dim_hidden\n",
        "            layers.append(nn.Linear(input_dim, output_dim))\n",
        "            if i < self.disc_layers:\n",
        "                layers.append(nn.LeakyReLU(0.2))\n",
        "                layers.append(nn.Dropout(self.disc_dropout))\n",
        "        layers.append(nn.Sigmoid())\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return self.layers(x).view(-1)\n",
        "      \n",
        "discriminator = Discriminator()\n",
        "\n",
        "src_emb.cuda()\n",
        "tgt_emb.cuda()\n",
        "mapping.cuda()\n",
        "discriminator.cuda()\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (layers): Sequential(\n",
              "    (0): Dropout(p=0.1)\n",
              "    (1): Linear(in_features=300, out_features=2048, bias=True)\n",
              "    (2): LeakyReLU(0.2)\n",
              "    (3): Dropout(p=0)\n",
              "    (4): Linear(in_features=2048, out_features=2048, bias=True)\n",
              "    (5): LeakyReLU(0.2)\n",
              "    (6): Dropout(p=0)\n",
              "    (7): Linear(in_features=2048, out_features=1, bias=True)\n",
              "    (8): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "metadata": {
        "id": "GZeIgNEF9Lmz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Use Stochastic Gradient Optimizer from Torch optim library to get source and target prediction and accuracy score**"
      ]
    },
    {
      "metadata": {
        "id": "GbPxakEj8bJH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "****"
      ]
    },
    {
      "metadata": {
        "id": "WlR41Zw0t3xe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optim_fn = optim.SGD\n",
        "optim_params = {'lr': 0.1}\n",
        "\n",
        "map_optimizer = optim_fn(mapping.parameters(), **optim_params)\n",
        "disc_optimizer = optim_fn(discriminator.layers.parameters(), **optim_params)\n",
        "epoch_size = 1000\n",
        "batch_size = 32\n",
        "disc_steps = 5\n",
        "disc_most_freq = 750\n",
        "disc_smooth = 0.1\n",
        "map_beta = 0.001\n",
        "\n",
        "def get_disc_xy(volatile):\n",
        "    \"\"\"\n",
        "    Get discriminator input batch / output target.\n",
        "    \"\"\"\n",
        "    # select random word IDs\n",
        "    bs = batch_size\n",
        "    mf = disc_most_freq\n",
        "    assert mf <= min(len(src_dico), len(tgt_dico))\n",
        "    src_ids = torch.FloatTensor(bs).random_(mf)\n",
        "    tgt_ids = torch.FloatTensor(bs).random_(mf)\n",
        "    #src_ids = src_ids.cuda()\n",
        "    #tgt_ids = tgt_ids.cuda()\n",
        "\n",
        "    # get word embeddings\n",
        "    src_emb = src_emb(Variable(src_ids, volatile=True))\n",
        "    tgt_emb = tgt_emb(Variable(tgt_ids, volatile=True))\n",
        "    src_emb = mapping(Variable(src_emb.data, volatile=volatile))\n",
        "    tgt_emb = Variable(tgt_emb.data, volatile=volatile)\n",
        "\n",
        "    # input / target\n",
        "    x = torch.cat([src_emb, tgt_emb], 0)\n",
        "    y = torch.FloatTensor(2 * bs).zero_()\n",
        "    y[:bs] = 1 - disc_smooth\n",
        "    y[bs:] = dis_smooth\n",
        "    y = Variable(y.cuda())\n",
        "\n",
        "    return x, y\n",
        "\n",
        "def eval_dis(self, to_log):\n",
        "  \n",
        "    bs = 128\n",
        "    src_preds = []\n",
        "    tgt_preds = []\n",
        "\n",
        "    discriminator.eval()\n",
        "\n",
        "    for i in range(0, src_emb.num_embeddings, bs):\n",
        "        emb = Variable(src_emb.weight[i:i + bs].data, volatile=True)\n",
        "        preds = discriminator(mapping(emb))\n",
        "        src_preds.extend(preds.data.cpu().tolist())\n",
        "\n",
        "    for i in range(0,tgt_emb.num_embeddings, bs):\n",
        "        emb = Variable(tgt_emb.weight[i:i + bs].data, volatile=True)\n",
        "        preds = discriminator(emb)\n",
        "        tgt_preds.extend(preds.data.cpu().tolist())\n",
        "\n",
        "    src_pred = np.mean(src_preds)\n",
        "    tgt_pred = np.mean(tgt_preds)\n",
        "    print(\"Discriminator source / target predictions: %.5f / %.5f\"\n",
        "                % (src_pred, tgt_pred))\n",
        "\n",
        "    src_accu = np.mean([x >= 0.5 for x in src_preds])\n",
        "    tgt_accu = np.mean([x < 0.5 for x in tgt_preds])\n",
        "    dis_accu = ((src_accu * src_emb.num_embeddings + tgt_accu * tgt_emb.num_embeddings) /\n",
        "                    (src_emb.num_embeddings + tgt_emb.num_embeddings))\n",
        "    print(\"Discriminator source / target / global accuracy: %.5f / %.5f / %.5f\"\n",
        "                    % (src_accu, tgt_accu, dis_accu))\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_cx6IswzrEfA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "13d3aaf4-ee81-4093-b4fe-7381a04c731d"
      },
      "cell_type": "code",
      "source": [
        " print(src_emb)\n",
        " print(tgt_emb)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 0.0645  0.0053  0.0004  ...   0.0030  0.0705 -0.0240\n",
            " 0.0620  0.0111 -0.0395  ...   0.1078 -0.0176 -0.0179\n",
            " 0.0003  0.0020 -0.0130  ...   0.1314  0.0437 -0.0296\n",
            "          ...             ⋱             ...          \n",
            "-0.0553 -0.0389  0.0185  ...   0.1073  0.0425 -0.0225\n",
            " 0.0567 -0.0338 -0.0031  ...   0.0750  0.0790 -0.0008\n",
            " 0.0381  0.0341 -0.0112  ...  -0.0733  0.0510  0.0021\n",
            "[torch.cuda.FloatTensor of size 332x300 (GPU 0)]\n",
            "\n",
            "\n",
            "-0.0033 -0.0060  0.0286  ...  -0.0333  0.0571  0.0010\n",
            " 0.0343 -0.0297  0.0183  ...   0.0736  0.0655  0.0099\n",
            "-0.0066  0.0013  0.0304  ...  -0.0067  0.0440 -0.0175\n",
            "          ...             ⋱             ...          \n",
            " 0.0438 -0.0602 -0.0035  ...  -0.0152  0.0350 -0.0371\n",
            "-0.0089 -0.0592 -0.0346  ...  -0.0275 -0.0218 -0.0437\n",
            "-0.0285 -0.0005  0.0397  ...  -0.0240  0.0102  0.0064\n",
            "[torch.cuda.FloatTensor of size 399x300 (GPU 0)]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zqW54Dpn9lSN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Adversarial Training**"
      ]
    },
    {
      "metadata": {
        "id": "K2Ax1lNBvtWx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def adv_training(epochs):\n",
        "    for epoch in range(epochs):\n",
        "        n_words_proc = 0\n",
        "        for n_iter in range(0,epoch_size, batch_size):\n",
        "            for _ in range(disc_steps): # Discriminator training\n",
        "                discriminator.train() # sets the module in training mode ex adds dropout and batchnorm\n",
        "                x,y = get_disc_xy(volatile=True)\n",
        "                preds = discriminator(Variable(x.data))\n",
        "                loss = F.binary_cross_entropy(preds,y)\n",
        "                disc_optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                disc_optimizer.step()\n",
        "                \n",
        "            \n",
        "            #Mapping step\n",
        "            discriminator.eval() # Puts the module in evaluation mode.\n",
        "            x, y = get_disc_xy(volatile=False)\n",
        "            preds = discriminator(Variable(x.data))\n",
        "            loss = F.binary_cross_entropy(preds, 1-y)\n",
        "            map_optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            map_optmizer.step()\n",
        "            orthogonalize()\n",
        "            n_words_proc += 2 * batch_size\n",
        "            \n",
        "        # embeddings / discriminator evaluation\n",
        "        eval_dis()\n",
        "\n",
        "\n",
        "exp_path = \"\"\n",
        "def export_embeddings(src_emb, tgt_emb, params):\n",
        "    \n",
        "    src_id2word = src_dico.id2word\n",
        "    tgt_id2word = tgt_dico.id2word\n",
        "    n_src = len(src_id2word)\n",
        "    n_tgt = len(tgt_id2word)\n",
        "    dim = src_emb.shape[1]\n",
        "    src_path = os.path.join(exp_path, 'vectors-%s.txt' % src_lang)\n",
        "    tgt_path = os.path.join(exp_path, 'vectors-%s.txt' % tgt_lang)\n",
        "    # source embeddings\n",
        "    with open(src_path, 'w') as f:\n",
        "        f.write(\"%i %i\\n\" % (n_src, dim))\n",
        "        for i in range(len(src_id2word)):\n",
        "            f.write(\"%s %s\\n\" % (src_id2word[i], \" \".join(str(x) for x in src_emb[i])))\n",
        "   # target embeddings\n",
        "    with open(tgt_path, 'w') as f:\n",
        "        f.write(\"%i %i\\n\" % (n_tgt, dim))\n",
        "        for i in range(len(tgt_id2word)):\n",
        "            f.write(\"%s %s\\n\" % (tgt_id2word[i], \" \".join(str(x) for x in tgt_emb[i])))\n",
        "            \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BN-HTLPpv9GD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "outputId": "abd836ac-00fe-440b-ebbe-6ade35e27154"
      },
      "cell_type": "code",
      "source": [
        "print(src_id2word)\n",
        "print(tgt_id2word)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: ',', 1: 'the', 2: '.', 3: 'and', 4: 'of', 5: 'to', 6: 'in', 7: 'a', 8: '\"', 9: ':', 10: ')', 11: 'that', 12: '(', 13: 'is', 14: 'for', 15: 'on', 16: '*', 17: 'with', 18: 'as', 19: 'it', 20: 'The', 21: 'or', 22: 'was', 23: \"'\", 24: \"'s\", 25: 'by', 26: 'from', 27: 'at', 28: 'I', 29: 'this', 30: 'you', 31: '/', 32: 'are', 33: '=', 34: 'not', 35: '-', 36: 'have', 37: '?', 38: 'be', 39: 'which', 40: ';', 41: 'all', 42: 'his', 43: 'has', 44: 'one', 45: 'their', 46: 'about', 47: 'but', 48: 'an', 49: '|', 50: 'said', 51: 'more', 52: 'page', 53: 'he', 54: 'your', 55: 'will', 56: 'its', 57: 'so', 58: 'were', 59: 'had', 60: 'also', 61: 'only', 62: 'if', 63: 'time', 64: 'some', 65: 'people', 66: 'like', 67: 'who', 68: 'them', 69: 'other', 70: 'they', 71: 'when', 72: 'Wikipedia', 73: 'article', 74: 'what', 75: '#', 76: 'just', 77: '!', 78: 'any', 79: 'after', 80: 'there', 81: 'would', 82: 'can', 83: 'In', 84: 'her', 85: 'talk', 86: 'use', 87: 'then', 88: 'into', 89: 'up', 90: '...', 91: 'we', 92: 'over', 93: 'my', 94: 'out', 95: 'here', 96: 'now', 97: 'because', 98: 'do', 99: 'work', 100: 'than', 101: 'no', 102: 'UTC', 103: 'me', 104: 'A', 105: 'two', 106: 'our', 107: 'been', 108: 'new', 109: 'where', 110: '–', 111: 'first', 112: 'such', 113: 'made', 114: '--', 115: 'If', 116: \"'t\", 117: 'both', 118: 'before', 119: 'way', 120: '1', 121: 'through', 122: 'information', 123: 'him', 124: 'being', 125: 'many', 126: 'most', 127: 'But', 128: 'those', 129: 'while', 130: 'name', 131: 'This', 132: 'should', 133: 'how', 134: 'even', 135: 'these', 136: 'see', 137: 'It', 138: 'make', 139: 'may', 140: '2', 141: 'under', 142: '—', 143: 's', 144: 'year', 145: '10', 146: 'state', 147: 'since', 148: 'years', 149: '”', 150: 'edit', 151: 'right', 152: 'did', 153: 'used', 154: 'government', 155: 'case', 156: 'articles', 157: 'get', 158: 'between', 159: 'she', 160: '“', 161: '}', 162: 'think', 163: '20', 164: 'during', 165: 'another', 166: 'And', 167: '11', 168: '3', 169: 'could', 170: 'using', 171: 'He', 172: 'without', 173: 'good', 174: 'place', 175: 'support', 176: 'back', 177: 'again', 178: 'against', 179: '12', 180: '15', 181: 'system', 182: 'called', 183: 'much', 184: 'still', 185: '18', 186: '%', 187: '16', 188: 'change', 189: 'say', 190: 'point', 191: 'us', 192: 'group', 193: 'day', 194: 'three', 195: '17', 196: '14', 197: 'know', 198: 'help', 199: 'As', 200: 'last', 201: '21', 202: 'each', 203: 'around', 204: '2008', 205: 'edits', 206: 'says', 207: '’', 208: '13', 209: '19', 210: 'life', 211: 'link', 212: '22', 213: 'too', 214: 'left', 215: '2009', 216: 'very', 217: 'students', 218: 'team', 219: 'sources', 220: 'off', 221: '2007', 222: 'well', 223: '4', 224: 'down', 225: 'section', 226: 'take', 227: 'does', 228: 'really', 229: '23', 230: 'discussion', 231: 'either', 232: 'show', 233: 'public', 234: '2010', 235: 'including', 236: 'set', 237: 'game', 238: 'policy', 239: 'later', 240: 'need', 241: 'program', 242: 'others', 243: 'source', 244: 'research', 245: '+', 246: 'area', 247: 'children', 248: 'why', 249: 'family', 250: 'making', 251: 'deletion', 252: 'something', 253: 'content', 254: 'school', 255: 'added', 256: 'found', 257: 'until', 258: 'within', 259: 'following', 260: 'better', 261: '5', 262: 'process', 263: '&', 264: 'history', 265: 'You', 266: 'given', 267: '2011', 268: 'question', 269: 'women', 270: 'For', 271: 'country', 272: 'company', 273: 'data', 274: 'part', 275: 'include', 276: 'site', 277: 'pages', 278: 'home', 279: 'business', 280: '$', 281: '2006', 282: 'having', 283: 'American', 284: '2012', 285: 'put', 286: 'issue', 287: 'different', 288: 'world', 289: 'U.S.', 290: 'New', 291: 'things', 292: 'list', 293: 'power', 294: 'above', 295: 'best', 296: 'future', 297: 'read', 298: 'report', 299: 'project', 300: 'find', 301: 'problem', 302: 'book', 303: 'though', 304: 'law', 305: 'today', 306: 'We', 307: 'four', 308: 'links', 309: 'community', 310: 'several', 311: 'film', 312: 'So', 313: '30', 314: 'free', 315: 'money', 316: 'line', 317: 'service', 318: 'actually', 319: '2013', 320: 'second', 321: 'go', 322: 'deleted', 323: 'editing', 324: 'often', 325: 'every', 326: '00', 327: 'course', 328: 'season', 329: 'possible', 330: 'That', 331: 'US'}\n",
            "{0: '.', 1: ',', 2: '</s>', 3: \"'\", 4: '-', 5: ')', 6: '(', 7: ':', 8: '/', 9: 'ஒரு', 10: '!', 11: '?', 12: 'என்று', 13: 'இந்த', 14: '}', 15: 'மற்றும்', 16: '\"', 17: '#', 18: '|', 19: 'இது', 20: 'என்ற', 21: '*', 22: 'நான்', 23: ';', 24: 'தான்', 25: '1', 26: 'அந்த', 27: 'வேண்டும்', 28: 'என', 29: 'என்பது', 30: 'உள்ள', 31: 'பல', 32: 'அல்லது', 33: '’', 34: '2', 35: 'ஆனால்', 36: 'என்', 37: 'அவர்', 38: 'இருந்து', 39: 'முதல்', 40: 'இருக்கும்', 41: '”', 42: 'அது', 43: 'சில', 44: 'தமிழ்', 45: 'ஆம்', 46: 'மக்கள்', 47: 'என்ன', 48: 'கொண்டு', 49: 'of', 50: '–', 51: 'ஆகும்', 52: 'இல்லை', 53: '%', 54: '�', 55: 'ஆண்டு', 56: 'the', 57: '“', 58: '‘', 59: 'உள்ளது', 60: 'at', 61: '3', 62: 'மேலும்', 63: 'போன்ற', 64: 'போது', 65: 'இந்திய', 66: 'அவர்கள்', 67: 'ஊராட்சி', 68: 'செய்து', 69: 'இவர்', 70: '5', 71: '7', 72: '=', 73: 'வரை', 74: 'மூலம்', 75: 'என்னும்', 76: '10', 77: '+', 78: 'வரும்', 79: 'அதன்', 80: 'வந்து', 81: 'இரண்டு', 82: 'எனக்கு', 83: '4', 84: 'to', 85: '\\\\', 86: 'கூட', 87: 'தன்', 88: 'and', 89: 'உங்கள்', 90: 'நன்றி', 91: 'பற்றி', 92: 'நாம்', 93: 'தனது', 94: 'பெரிய', 95: 'மட்டும்', 96: 'கொண்ட', 97: 'நீங்கள்', 98: 'என்றும்', 99: 'அரசு', 100: 'மிகவும்', 101: 'in', 102: 'அதை', 103: 'இல்', 104: 'நல்ல', 105: 'போல', 106: '…', 107: 'எந்த', 108: 'புதிய', 109: 'இருந்தது', 110: 'எல்லாம்', 111: 'நாள்', 112: '6', 113: 'பின்னர்', 114: '&', 115: 'செய்ய', 116: 'இருக்கிறது', 117: 'இன்னும்', 118: 'மட்டுமே', 119: '12', 120: 'ஒன்று', 121: 'இருந்த', 122: 'அவன்', 123: 'பிறகு', 124: 'படம்', 125: 'எப்படி', 126: 'மிக', 127: '11', 128: 'இதன்', 129: '8', 130: 'அவரது', 131: 'உள்ளன', 132: 'என்பதை', 133: 'மூன்று', 134: ']', 135: 'பின்', 136: 'வந்த', 137: 'மீது', 138: 'விட்டு', 139: 'முடியும்', 140: 'தொகை', 141: '[', 142: 'மீண்டும்', 143: 'முறை', 144: 'பெயர்', 145: 'இன்று', 146: 'ஒரே', 147: 'வேறு', 148: 'இதில்', 149: 'பெண்கள்', 150: 'இங்கு', 151: 'தொடர்ந்து', 152: 'நீ', 153: 'அன்று', 154: 'கோயில்', 155: 'ஒவ்வொரு', 156: 'a', 157: 'என்றால்', 158: 'ஏன்', 159: '2013', 160: '9', 161: 'பதிவு', 162: 'ஆகிய', 163: 'வைத்து', 164: 'சென்று', 165: '20', 166: 'அரசியல்', 167: 'ஆண்டில்', 168: 'போல்', 169: '2011', 170: 'வேலை', 171: 'அதில்', 172: '15', 173: 'அதே', 174: 'மாவட்டம்', 175: 'மாதிரி', 176: 'பற்றிய', 177: '2012', 178: 'அவள்', 179: 'நம்', 180: 'இரு', 181: 'நேரம்', 182: 'உண்டு', 183: 'on', 184: 'இதை', 185: 'இப்படி', 186: 'by', 187: 'விட', 188: 'இருக்க', 189: '2017', 190: 'அமைந்துள்ளது', 191: '2016', 192: 'என்னை', 193: 'எனும்', 194: 'கலந்து', 195: 'மேல்', 196: 'முக்கிய', 197: 'அடுத்த', 198: 'பெண்', 199: 'தேசிய', 200: 'கொஞ்சம்', 201: '2015', 202: 'செய்யும்', 203: '00', 204: 'சென்னை', 205: 'முன்', 206: 'Tamil', 207: '2014', 208: 'சரி', 209: 'பேரும்', 210: 'இப்போது', 211: 'பேர்', 212: 'அனைத்து', 213: 'அதிக', 214: 'நிலையில்', 215: 'இடம்', 216: 'இவை', 217: 'ஓர்', 218: 'கல்வி', 219: 'முடியாது', 220: 'பல்வேறு', 221: '14', 222: 'for', 223: '16', 224: '30', 225: 'நிலை', 226: 'மாவட்டத்தில்', 227: 'அமைந்துள்ள', 228: 'காலத்தில்', 229: 'சிறந்த', 230: 'கதை', 231: 'காரணம்', 232: '13', 233: 'இந்தப்', 234: 'எனது', 235: 'தங்கள்', 236: '@', 237: 'அதற்கு', 238: 'கொள்ள', 239: 'பார்த்து', 240: 'எனவே', 241: 'இலங்கை', 242: 'முதலில்', 243: 'இந்தியா', 244: 'சொல்லி', 245: '18', 246: 'அமெரிக்க', 247: 'அவை', 248: 'போய்', 249: 'காலம்', 250: 'The', 251: 'ஒருவர்', 252: 'துடுப்பாட்டப்', 253: 'தொகுதிக்கும்', 254: 'பெற்ற', 255: 'மற்ற', 256: '17', 257: 'is', 258: 'மாதம்', 259: 'கடந்த', 260: 'சேர்ந்த', 261: '>', 262: 'உலக', 263: 'வருகிறது', 264: 'பெரும்', 265: 'மொத்த', 266: 'எல்லா', 267: '25', 268: '19', 269: 'அவர்களின்', 270: '2010', 271: 'உடல்', 272: 'அதிகம்', 273: 'மணி', 274: 'இங்கே', 275: 'சொல்ல', 276: 'வேண்டிய', 277: 'மே', 278: 'இருந்தாலும்', 279: 'தற்போது', 280: 'நான்கு', 281: 'இதனால்', 282: 'மொழி', 283: 'வந்தது', 284: 'கொண்டுள்ளது', 285: 'எடுத்து', 286: 'பகுதியில்', 287: 'வெற்றி', 288: 'யார்', 289: 'உங்களுக்கு', 290: 'போட்டு', 291: 'குறித்து', 292: 'ல்', 293: 'மத்திய', 294: 'இருந்தால்', 295: '0', 296: 'ரொம்ப', 297: 'அங்கு', 298: 'உள்ளனர்', 299: 'said', 300: 'பகுதி', 301: 'அளவு', 302: 'அப்படி', 303: 'பார்க்க', 304: 'பொருள்', 305: 'வகையில்', 306: 'ஊரில்', 307: 'பிற', 308: 'அதனால்', 309: 'http', 310: 'செய்த', 311: 'காரணமாக', 312: 'தலைவர்', 313: 'இருக்கு', 314: 'இந்து', 315: '21', 316: 'பொது', 317: 'தவிர', 318: 'உன்', 319: 'அப்போது', 320: 'இல்லாமல்', 321: 'நமது', 322: 'காதல்', 323: 'தமிழ்நாட்டில்', 324: 'நீர்', 325: 'செய்திகள்', 326: '22', 327: 'அல்ல', 328: 'பதில்', 329: 'ஆண்கள்', 330: 'நிறைய', 331: 'ம்', 332: '\\u200b', 333: 'நீங்க', 334: 'அதாவது', 335: 'பொதுவாக', 336: 'அறிவியல்', 337: 'நானும்', 338: 'வழிமாற்று', 339: 'ஆறு', 340: '24', 341: 'இடத்தில்', 342: 'தகவல்', 343: 'சார்', 344: 'அளவில்', 345: 'தமிழக', 346: 'மனித', 347: 'இந்தியாவின்', 348: 'இரண்டாம்', 349: 'சட்டமன்றத்', 350: '2009', 351: 'சிறு', 352: 'ஆசிரியர்', 353: 'இந்தக்', 354: '23', 355: 'தமிழ்நாடு', 356: 'பாடல்', 357: 'அவருக்கு', 358: 'கீழ்', 359: 'இதற்கு', 360: 'UTC', 361: 'கிடைக்கும்', 362: 'ஆவார்', 363: 'மொத்தம்', 364: 'பக்கம்', 365: 'சார்ந்த', 366: 'வாங்க', 367: 'வாழ்க்கை', 368: 'பழைய', 369: 'நூல்', 370: 'கவிதை', 371: 'செல்லும்', 372: 'ஏற்படும்', 373: 'நேரத்தில்', 374: 'முறையில்', 375: 'தேர்வு', 376: 'தமிழில்', 377: 'சிறிய', 378: 'சமூக', 379: 'ஆக', 380: 'ஐந்து', 381: 'வழி', 382: 'எங்கள்', 383: 'பி', 384: 'சுமார்', 385: 'சேர்த்து', 386: 'இவர்கள்', 387: 'கருத்து', 388: '»', 389: 'A', 390: 'என்றார்', 391: 'இவற்றில்', 392: 'பேச்சு', 393: 'இந்தியாவில்', 394: 'அம்மா', 395: 'ஐக்கிய', 396: 'சிறப்பு', 397: 'அவரை', 398: 'மக்களின்'}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}